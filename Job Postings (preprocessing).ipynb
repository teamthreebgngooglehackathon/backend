{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "from os import listdir\n",
    "import glob\n",
    "from scipy import spatial\n",
    "import spacy\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from genism.models import Word2Vec\n",
    "#print(os.listdir(\"../input\"))\n",
    "#print(os.listdir(\"..\"))\n",
    "#import spacy\n",
    "#from spacy import displacy\n",
    "#from topia.termextract import extract\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a third parameter(flag) as 1 in the match_profile() in order to get your recommendations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4827ed5e92d47eaa059096e4edea8afcd2a6e06"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(arr1,arr2):\n",
    "    ans=1- spatial.distance.cosine(arr1,arr2)\n",
    "    if(np.isnan(ans)):\n",
    "        return 0\n",
    "    else:\n",
    "        return ans\n",
    "class job_postings:    \n",
    "    def __init__(self,link):\n",
    "        self.df2=pd.read_csv(link)\n",
    "        self.training_range=int(len(self.df2.loc[:,'uniq_id']))\n",
    "    def check_threshold(threshold,ele):\n",
    "        if(ele[0]!=threshold[0][0] and abs(ele[1]-threshold[0][1])<0.03):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def categorize_jobs(self):\n",
    "        # #Predefined categories\n",
    "        #Compare similarities of word embeddings\n",
    "        nlp=spacy.load('en_core_web_lg')\n",
    "        job_id=self.df2.loc[:,'uniq_id'].tolist()[:self.training_range]\n",
    "        job_titles=self.df2.loc[:,'jobtitle'].tolist()[:self.training_range]\n",
    "        job_descriptions=self.df2.loc[:,'jobdescription'].tolist()[:self.training_range]\n",
    "        final_cat=pd.DataFrame(index=job_id)\n",
    "        #categories=['Network Engineer','Application Development','Big Data','Data Analyst','Software Developer','DevOps','Software Testing','Front End','Back End','Full Stack','Web Development','Information Security','Mobile developer','System Administrator','Business Analyst','Manager','Cloud']\n",
    "        categories=['Network Engineer','Full stack','QA/Test Developer','Enterprise application','DevOps','Mobile Developer','Back End','Database Administrator(DBA)','Front End','Game developer','System Administrator','Data Scientist','Business analyst','Sales professional','Product Manager','Information Security','Software Developer/Java Developer','Web Developer','Cloud Computing']\n",
    "        for category in categories:\n",
    "            final_cat[category]=np.nan\n",
    "        for job_t_d in list(zip(job_id,job_titles,job_descriptions)):\n",
    "            id_job=job_t_d[0]\n",
    "            job_i=job_t_d[1]\n",
    "            job_d=job_t_d[2]\n",
    "            job_title=nlp(job_i.lower())\n",
    "            job_description=nlp(job_d.lower())\n",
    "            match_cat_title=dict()\n",
    "            match_cat_description=dict()\n",
    "            for category in categories:\n",
    "                word=nlp(category.lower())\n",
    "                match_cat_title[category]=job_title.similarity(word)\n",
    "                match_cat_description[category]=job_description.similarity(word)\n",
    "            match_cat_title=sorted(match_cat_title.items(),key=lambda x:x[1],reverse=True)\n",
    "            match_cat_description=sorted(match_cat_description.items(),key=lambda x:x[1],reverse=True)\n",
    "\n",
    "\n",
    "            #a represents max\n",
    "            #if(match_cat_title[0][1]>0.5 or match_cat_description[0][1]>0.5):\n",
    "            a=match_cat_title[0]\n",
    "            #print(a)\n",
    "            match_cat_description=list(filter(lambda x: self.check_threshold(match_cat_title,x),match_cat_description))\n",
    "            if(len(match_cat_description)!=0):\n",
    "                print(match_cat_description)\n",
    "                print(id_job)\n",
    "                #b=match_cat_description[0]\n",
    "                final_cat.loc[id_job,a[0]]=1\n",
    "                match_cat_description.extend([(match_cat_title[0][0],1)])\n",
    "                sum_proportion=sum([x[1] for x in match_cat_description])\n",
    "                for ele in match_cat_description:\n",
    "                    final_cat.loc[id_job,ele[0]]=ele[1]/sum_proportion\n",
    "            else:\n",
    "                print(id_job)\n",
    "                final_cat.loc[id_job,a[0]]=1\n",
    "        return final_cat\n",
    "    def clean_skills(self):\n",
    "        extracted_skills=dict()\n",
    "        job_skills=np.asarray(self.df2.loc[:,\"skills\"])\n",
    "        for i in range(self.training_range):\n",
    "            #print(i)\n",
    "            #Method 1: Manual pre-processing\n",
    "            job_id=self.df2.iloc[i,-1]\n",
    "            #Method 2:Using NLTK\n",
    "            tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "            #print(job_skills[i])\n",
    "            if(pd.isnull(job_skills[i])):\n",
    "                continue\n",
    "            stopwords_list=stopwords.words(\"english\")\n",
    "            tokens=re.split(\"|\".join([\",\",\" and\",\"/\",\" AND\",\" or\",\" OR\",\";\"]),job_skills[i])\n",
    "            tokens=list(set(tokens))\n",
    "            extracted_skills[job_id]=[]\n",
    "            extracted_skills[job_id].extend(tokens)\n",
    "        return extracted_skills\n",
    "    def extract_skills(self,extracted_skills):\n",
    "        df_languages=pd.read_excel('resultsjob_profile/languages.xlsx')\n",
    "        df_frameworks=pd.read_csv(\"results/frameworks.csv\")\n",
    "        df_database=pd.read_csv(\"results/database.csv\")\n",
    "        df_os=pd.read_csv(\"results/operating_systems.csv\")\n",
    "        df_plat=pd.read_csv(\"results/platforms.csv\")\n",
    "        frameworks=df_frameworks.iloc[:,1].tolist()\n",
    "        frameworks=[x.lower().strip() for x in frameworks]\n",
    "        #frameworks=[str(x).split(\",\")[0] for x in df_frameworks.iloc[:,1]]\n",
    "        languages=list(df_languages.iloc[:,0])\n",
    "        languages=[x.lower().strip() for x in languages]\n",
    "        #frameworks=[x.lower().strip().split('\\t')[0] for x in frameworks]\n",
    "        databases=df_database.iloc[:,0].tolist()\n",
    "        databases=[x.lower().strip() for x in databases]\n",
    "        op_systems=df_os.iloc[:,0].tolist()\n",
    "        op_systems=[x.lower().strip() for x in op_systems]\n",
    "        platforms=df_plat.iloc[:,1].tolist()\n",
    "        #print(platforms)\n",
    "        platforms=[x.lower().strip() for x in platforms]\n",
    "        #print(frameworks)\n",
    "        new_extracted=dict()\n",
    "        for ele in extracted_skills.keys():\n",
    "            final_lang=''\n",
    "            final_frame=''\n",
    "            final_others=''\n",
    "            final_database=''\n",
    "            final_plat=''\n",
    "            final_os=''\n",
    "            #print(extracted_skills[ele])\n",
    "            for skill in extracted_skills[ele]:\n",
    "                skill_base=skill.lower().strip()\n",
    "                #print(skill_base)\n",
    "                if(skill_base in languages):\n",
    "                    if(final_lang==''):\n",
    "                        final_lang=skill_base\n",
    "                    else:\n",
    "                        final_lang=final_lang+\",\"+skill_base\n",
    "                elif(skill_base in frameworks):\n",
    "                    if(final_frame==''):\n",
    "                        final_frame=skill_base\n",
    "                    else:\n",
    "                        final_frame=final_frame+\",\"+skill_base\n",
    "                elif(skill_base in databases):\n",
    "                    if(final_database==''):\n",
    "                        final_database=skill_base\n",
    "                    else:\n",
    "                        final_database=final_database+\",\"+skill_base\n",
    "                elif(skill_base in op_systems):\n",
    "                    if(final_os==''):\n",
    "                        final_os=skill_base\n",
    "                    else:\n",
    "                        final_os=final_os+\",\"+skill_base\n",
    "                elif(skill_base in platforms):\n",
    "                    if(final_plat==''):\n",
    "                        final_plat=skill_base\n",
    "                    else:\n",
    "                        final_plat=final_plat+\",\"+skill_base\n",
    "                else:\n",
    "                    if(final_others==''):\n",
    "                        final_others=skill_base\n",
    "                    else:\n",
    "                        final_others=final_others+\",\"+skill_base\n",
    "            new_extracted[ele]=[final_lang,final_frame,final_database,final_os,final_plat,final_others]\n",
    "        print((list(new_extracted.items()))[:100])\n",
    "        for ele,describe in list(zip(self.df2.loc[:,'uniq_id'],self.df2.loc[:,'jobdescription'].tolist()))[:self.training_range]:\n",
    "            doc=nlp(describe)\n",
    "            final_lang=''\n",
    "            final_frame=''\n",
    "            final_others=''\n",
    "            final_database=''\n",
    "            final_plat=''\n",
    "            final_os=''\n",
    "            for ent in doc.ents:\n",
    "                word=ent.text\n",
    "                word=word.lower().strip()\n",
    "                if(word in languages and word not in final_lang and word not in new_extracted[ele][0].split(\",\")):\n",
    "                    if(final_lang==''):\n",
    "                        final_lang=word\n",
    "                    else:\n",
    "                        final_lang=final_lang+\",\"+word\n",
    "                elif(word in frameworks and word not in final_frame and word not in new_extracted[ele][1].split(\",\")):\n",
    "                    if(final_frame==''):\n",
    "                        final_frame=word\n",
    "                    else:\n",
    "                        final_frame=final_frame+\",\"+word\n",
    "                elif(word in databases and word not in final_database and word not in new_extracted[ele][2].split(\",\")):\n",
    "                    if(final_database==''):\n",
    "                        final_database=word\n",
    "                    else:\n",
    "                        final_database=final_database+\",\"+word\n",
    "                elif(word in op_systems and word not in final_os and word not in new_extracted[ele][3].split(\",\")):\n",
    "                    if(final_os==''):\n",
    "                        final_os=word\n",
    "                    else:\n",
    "                        final_os=final_os+\",\"+word\n",
    "                elif(word in platforms and word not in final_plat and word not in new_extracted[ele][4].split(\",\")):\n",
    "                    if(final_plat==''):\n",
    "                        final_plat=word\n",
    "                    else:\n",
    "                        final_plat=final_plat+\",\"+word\n",
    "                else:\n",
    "                    if(final_others==''):\n",
    "                        final_others=word\n",
    "                    else:\n",
    "                        final_others=final_others+\",\"+word\n",
    "            if(final_lang!=''):\n",
    "                new_extracted[ele][0]+=\",\"+final_lang\n",
    "            if(final_frame!=''):\n",
    "                new_extracted[ele][1]+=\",\"+final_frame\n",
    "            if(final_database!=''):\n",
    "                new_extracted[ele][2]+=\",\"+final_database\n",
    "            if(final_os!=''):\n",
    "                new_extracted[ele][3]+=\",\"+final_os\n",
    "            if(final_plat!=''):\n",
    "                new_extracted[ele][4]+=\",\"+final_plat\n",
    "            if(final_others!=''):\n",
    "                new_extracted[ele][5]+=\",\"+final_others\n",
    "            #new_extracted[ele]=[final_lang,final_frame,final_database,final_os,final_plat,final_others]\n",
    "        extracted_skills_df=pd.DataFrame.from_dict(new_extracted,orient='index',columns=['Language','Framework','Database','OS','Platform','Others'])\n",
    "        return extracted_skills_df\n",
    "    def create_job_profile(self,extracted_skills_df,domain_df):\n",
    "        job_id=extracted_skills_df.index.tolist()\n",
    "        languages_df=pd.DataFrame(index=job_id)\n",
    "        platforms_df=pd.DataFrame(index=job_id)\n",
    "        frameworks_df=pd.DataFrame(index=job_id)\n",
    "        databases_df=pd.DataFrame(index=job_id)\n",
    "        \n",
    "        for job,lang,frame,plat,datab in list(zip(job_id,extracted_skills_df.loc[:,'Language'].tolist(),extracted_skills_df.loc[:,'Framework'].tolist(),extracted_skills_df.loc[:,'Platform'].tolist(),extracted_skills_df.loc[:,'Database'].tolist())):\n",
    "            #Languages\n",
    "            l=lang.split(\",\")\n",
    "            if(lang!=np.nan or lang!=''):\n",
    "                for ele in l:\n",
    "                    if(ele==''):\n",
    "                        continue\n",
    "                    if(ele not in languages_df.columns):\n",
    "                        #languages.append(ele)\n",
    "                        languages_df[ele]=np.nan\n",
    "                    languages_df.loc[job,ele]=1\n",
    "            \n",
    "            #Frameworks\n",
    "            l=frame.split(\",\")\n",
    "            if(frame!=np.nan or frame!=''):\n",
    "                for ele in l:\n",
    "                    if(ele==''):\n",
    "                        continue\n",
    "                    if(ele not in frameworks_df.columns):\n",
    "                        #languages.append(ele)\n",
    "                        frameworks_df[ele]=np.nan\n",
    "                    frameworks_df.loc[job,ele]=1\n",
    "\n",
    "            #Platforms\n",
    "            l=plat.split(\",\")\n",
    "            if(plat!=np.nan or plat!=''):\n",
    "                for ele in l:\n",
    "                    if(ele==''):\n",
    "                        continue\n",
    "                    if(ele not in platforms_df.columns):\n",
    "                        #languages.append(ele)\n",
    "                        platforms_df[ele]=np.nan\n",
    "                    platforms_df.loc[job,ele]=1\n",
    "            \n",
    "            #Databases\n",
    "            l=datab.split(\",\")\n",
    "            if(datab!=np.nan or datab!=''):\n",
    "                for ele in l:\n",
    "                    if(ele==''):\n",
    "                        continue\n",
    "                    if(ele not in databases_df.columns):\n",
    "                        #languages.append(ele)\n",
    "                        databases_df[ele]=np.nan\n",
    "                    databases_df.loc[job,ele]=1\n",
    "        languages_df=languages_df.reindex_axis(sorted(languages_df.columns), axis=1)\n",
    "        frameworks_df=frameworks_df.reindex_axis(sorted(frameworks_df.columns), axis=1)\n",
    "        platforms_df=platforms_df.reindex_axis(sorted(platforms_df.columns), axis=1)\n",
    "        databases_df=databases_df.reindex_axis(sorted(databases_df.columns), axis=1)\n",
    "        domain_df=domain_df.reindex_axis(sorted(domain_df.columns), axis=1)\n",
    "        \n",
    "        languages_df.index.name=frameworks_df.index.name=platforms_df.index.name=databases_df.index.name=domain_df.index.name='uniq_id'\n",
    "        languages_df.to_csv(\"results/languages_job_profile.csv\")\n",
    "        frameworks_df.to_csv(\"results/frameworks_job_profile.csv\")\n",
    "        platforms_df.to_csv(\"results/platforms_job_profile.csv\")\n",
    "        databases_df.to_csv(\"results/databases_job_profile.csv\")\n",
    "        domain_df.to_csv(\"results/domain_job_profile.csv\")\n",
    "        print(languages_df.columns)\n",
    "        \n",
    "    def clean_common_profile(self,df_user,df_job,flag):\n",
    "        #Shift .net from languages to frameworks\n",
    "        if(flag=='Language'):\n",
    "            print(df_job.columns.tolist())\n",
    "            #bash and bash/shell\n",
    "            count=0\n",
    "            for ele in df_user.loc[:,'bash/shell']:\n",
    "                if(ele==1.0):\n",
    "                    df_user.ix[count,'bash']=1.0\n",
    "                count=count+1\n",
    "            df_user=df_user.drop('bash/shell',axis=1)\n",
    "            count=0\n",
    "            for ele in df_job.loc[:,'bash/shell']:\n",
    "                if(ele==1.0):\n",
    "                    df_job.ix[count,'bash']=1.0\n",
    "                count=count+1\n",
    "            df_job=df_job.drop('bash/shell',axis=1)\n",
    "\n",
    "        if(flag=='Framework'):\n",
    "            print(df_user.columns.tolist())\n",
    "            count=0\n",
    "            for ele in df_user.loc[:,'nodejs']:\n",
    "                if(ele==1.0):\n",
    "                    df_user.ix[count,'node.js']=1.0\n",
    "                count=count+1\n",
    "            df_user=df_user.drop('nodejs',axis=1)\n",
    "            count=0\n",
    "            for ele in df_job.loc[:,'nodejs']:\n",
    "                if(ele==1.0):\n",
    "                    df_job.ix[count,'node.js']=1.0\n",
    "                count=count+1\n",
    "            df_job=df_job.drop('nodejs',axis=1)\n",
    "            \n",
    "            count=0\n",
    "            for ele in df_user.loc[:,'angularjs']:\n",
    "                if(ele==1.0):\n",
    "                    df_user.ix[count,'angular']=1.0\n",
    "                count=count+1\n",
    "            df_user=df_user.drop('angularjs',axis=1)\n",
    "            count=0\n",
    "            for ele in df_job.loc[:,'angularjs']:\n",
    "                if(ele==1.0):\n",
    "                    df_job.ix[count,'angular']=1.0\n",
    "                count=count+1\n",
    "            df_job=df_job.drop('angularjs',axis=1)\n",
    "            \n",
    "        if(flag=='Platform'):\n",
    "            print(df_user.columns.tolist())\n",
    "        if(flag=='Database'):\n",
    "            print(df_user.columns.tolist())\n",
    "            count=0\n",
    "            for ele in df_user.loc[:,'microsoft sql server']:\n",
    "                if(ele==1.0):\n",
    "                    df_user.ix[count,'sql server']=1.0\n",
    "                count=count+1\n",
    "            df_user=df_user.drop('microsoft sql server',axis=1)\n",
    "            count=0\n",
    "            for ele in df_job.loc[:,'microsoft sql server']:\n",
    "                if(ele==1.0):\n",
    "                    df_job.ix[count,'sql server']=1.0\n",
    "                count=count+1\n",
    "            df_job=df_job.drop('microsoft sql server',axis=1)\n",
    "        return df_user,df_job\n",
    "\n",
    "    #Input is two dataframes    \n",
    "    def create_common_profile(self,job_profile_path,user_profile_path,output_path,flag=0):\n",
    "        if(flag==0):\n",
    "            #Domain\n",
    "            userprofile=pd.read_csv(user_profile_path+\"DevType.csv\",index_col='Respondent')\n",
    "            jobprofile=pd.read_csv(job_profile_path+\"domain_job_profile.csv\",index_col='Unnamed: 0')\n",
    "            print(\"Read from file\")\n",
    "            print(jobprofile.index)\n",
    "            #jobprofile=jobprofile.reset_index()\n",
    "            #userprofile=userprofile.reset_index()\n",
    "            userprofile.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            jobprofile.drop('uniq_id', axis=1, inplace=True)\n",
    "            jobprofile.index.name='uniq_id'\n",
    "            print(\"index 2in domain\")\n",
    "            print(jobprofile.index)\n",
    "            #print(jobprofile.loc[:,'uniq_id'])\n",
    "            userprofile.rename(columns={'Product manager':'Product Manager','Back-end developer':'Back End','C-suite executive (CEO, CTO, etc.)':'C-suite executive','Data scientist or machine learning specialist':'Data Scientist','Database administrator':'Database Administrator(DBA)','Mobile developer':'Mobile Developer','Desktop or enterprise applications developer':'Enterprise application','DevOps specialist':'DevOps','Front-end developer':'Front End','Full-stack developer':'Full stack','Marketing or sales professional':'Sales professional','QA or test developer':'QA/Test Developer','System administrator':'System Administrator','Game or graphics developer':'Game developer'},inplace=True)\n",
    "            jobprofile.rename(columns={'Business analyst':'Data or business analyst'},inplace=True)\n",
    "            print(userprofile.columns)\n",
    "            print(jobprofile.columns)\n",
    "            print(\"index in domain\")\n",
    "            print(jobprofile.index)\n",
    "            #Present in userprofile but not in jobprofile\n",
    "            a=list(set(userprofile.columns)-set(jobprofile.columns))\n",
    "            print(a)\n",
    "            for i in a:\n",
    "                if(i!='Respondent'):\n",
    "                    jobprofile[i]=0\n",
    "            b=list(set(jobprofile.columns)-set(userprofile.columns))\n",
    "            print(b)\n",
    "            for i in b:\n",
    "                if(i!='uniq_id'):\n",
    "                    userprofile[i]=0\n",
    "            #userprofile=userprofile.set_index('Respondent')\n",
    "            #jobprofile=jobprofile.set_index('uniq_id')\n",
    "            userprofile=userprofile[sorted(userprofile.columns.tolist())]\n",
    "            jobprofile=jobprofile[sorted(jobprofile.columns.tolist())]\n",
    "            #Exclude \n",
    "\n",
    "            print(userprofile.columns==jobprofile.columns)\n",
    "\n",
    "            print(userprofile.columns)\n",
    "            print(jobprofile.columns)\n",
    "            userprofile=userprofile[userprofile.columns.tolist()]\n",
    "            jobprofile=jobprofile[jobprofile.columns.tolist()]\n",
    "            userprofile.to_csv(output_path+\"domain_user_profile.csv\")\n",
    "            jobprofile.to_csv(output_path+\"domain_job_profile.csv\")\n",
    "\n",
    "            #Languages\n",
    "            df_user=pd.read_csv(user_profile_path+\"LanguageWorkedWith.csv\",index_col='Respondent')\n",
    "            df_job=pd.read_csv(job_profile_path+\"languages_job_profile.csv\",index_col=0)\n",
    "            df_job.index.name='uniq_id'\n",
    "            print(\"index is\")\n",
    "            print(df_job.index)\n",
    "            print(df_user.columns)\n",
    "            print(df_job.columns)\n",
    "            df_user.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            df_job.rename(columns={'visual basic .net':'vb.net'},inplace=True)\n",
    "            df_user.columns=list(map(lambda x:x.lower(),df_user.columns))\n",
    "            df_job.columns=list(map(lambda x:x.lower(),df_job.columns))\n",
    "            columns_to_add=[]\n",
    "            a=list(set(df_user.columns)-(set(df_job.columns)))\n",
    "            print(a)\n",
    "            for i in a:\n",
    "                if(i!='Respondent'):\n",
    "                    df_job[i]=0        \n",
    "            b=list(set(df_job.columns)-set(df_user.columns))\n",
    "            print(b)\n",
    "            for i in b:\n",
    "                if(i!='uniq_id'):\n",
    "                    df_user[i]=0\n",
    "            print(df_job.index)        \n",
    "            df_user=df_user[sorted(df_user.columns.tolist())]\n",
    "            df_job=df_job[sorted(df_job.columns.tolist())]\n",
    "            #df_user=userprofile.reindex_axis(sorted(df_user.columns), axis=1)\n",
    "            #df_job=jobprofile.reindex_axis(sorted(df_job.columns), axis=1)\n",
    "            print(\"index 2\")\n",
    "            print(df_job.index)\n",
    "            print(len(set(df_user.columns).intersection(df_job.columns)),len(df_user.columns))\n",
    "            df_user,df_job=self.clean_common_profile(df_user,df_job,'Language')\n",
    "            print(\"language is\")\n",
    "            print(df_job.index[0])\n",
    "            print(df_job.loc[df_job.index[0],:])\n",
    "            df_user.to_csv(output_path+\"languages_profile_user.csv\")\n",
    "            df_job.to_csv(output_path+\"languages_profile_job.csv\")\n",
    "\n",
    "            #Frameworks\n",
    "            df_user=pd.read_csv(user_profile_path+\"FrameworkWorkedWith.csv\",index_col='Respondent')\n",
    "            df_job=pd.read_csv(job_profile_path+\"frameworks_job_profile.csv\",index_col=0) \n",
    "            df_job.index.name='uniq_id'\n",
    "            print(df_user.columns)\n",
    "            print(df_job.columns)\n",
    "            df_user.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.rename(columns={'visual basic .net':'vb.net'},inplace=True)\n",
    "            df_user.columns=list(map(lambda x:x.lower(),df_user.columns))\n",
    "            df_job.columns=list(map(lambda x:x.lower(),df_job.columns))\n",
    "\n",
    "            a=list(set(df_user.columns)-(set(df_job.columns)))\n",
    "            print(a)\n",
    "            for i in a:\n",
    "                if(i!='Respondent'):\n",
    "                    df_job[i]=0        \n",
    "            b=list(set(df_job.columns)-set(df_user.columns))\n",
    "            print(b)\n",
    "            for i in b:\n",
    "                if(i!='uniq_id'):\n",
    "                    df_user[i]=0\n",
    "            #userprofile=userprofile.reindex_axis(sorted(userprofile.columns), axis=1)\n",
    "            #jobprofile=jobprofile.reindex_axis(sorted(jobprofile.columns), axis=1)\n",
    "            df_user=df_user[sorted(df_user.columns.tolist())]\n",
    "            df_job=df_job[sorted(df_job.columns.tolist())]\n",
    "\n",
    "            print(len(set(df_user.columns).intersection(df_job.columns)),len(df_user.columns))\n",
    "            df_user,df_job=self.clean_common_profile(df_user,df_job,'Framework')   \n",
    "            df_user.to_csv(output_path+\"frameworks_profile_user.csv\")\n",
    "            df_job.to_csv(output_path+\"frameworks_profile_job.csv\")\n",
    "\n",
    "            #Platforms\n",
    "            df_user=pd.read_csv(user_profile_path+\"PlatformWorkedWith.csv\",index_col='Respondent')\n",
    "            df_job=pd.read_csv(job_profile_path+\"platforms_job_profile.csv\",index_col=0) \n",
    "            print(df_user.columns)\n",
    "            df_job.index.name='uniq_id'\n",
    "            print(df_job.columns)\n",
    "            df_user.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.rename(columns={'visual basic .net':'vb.net'},inplace=True)\n",
    "            df_user.columns=list(map(lambda x:x.lower(),df_user.columns))\n",
    "            df_job.columns=list(map(lambda x:x.lower(),df_job.columns))\n",
    "\n",
    "            a=list(set(df_user.columns)-(set(df_job.columns)))\n",
    "            print(a)\n",
    "            for i in a:\n",
    "                if(i!='Respondent'):\n",
    "                    df_job[i]=0\n",
    "            b=list(set(df_job.columns)-set(df_user.columns))\n",
    "            print(b)\n",
    "            for i in b:\n",
    "                if(i!='uniq_id'):\n",
    "                    df_user[i]=0\n",
    "            df_user=df_user[sorted(df_user.columns.tolist())]\n",
    "            df_job=df_job[sorted(df_job.columns.tolist())]\n",
    "\n",
    "            print(len(set(df_user.columns).intersection(df_job.columns)),len(df_user.columns))\n",
    "            df_user,df_job=self.clean_common_profile(df_user,df_job,'Platform')        \n",
    "            df_user.to_csv(output_path+\"platforms_profile_user.csv\")\n",
    "            df_job.to_csv(output_path+\"platforms_profile_job.csv\")\n",
    "\n",
    "            #Databases\n",
    "            df_user=pd.read_csv(user_profile_path+\"DatabaseWorkedWith.csv\",index_col='Respondent')\n",
    "            df_job=pd.read_csv(job_profile_path+\"databases_job_profile.csv\",index_col=0) \n",
    "            df_job.index.name='uniq_id'\n",
    "            print(df_user.columns)\n",
    "            print(df_job.columns)\n",
    "            df_user.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "            #df_job.rename(columns={'visual basic .net':'vb.net'},inplace=True)\n",
    "            df_user.columns=list(map(lambda x:x.lower(),df_user.columns))\n",
    "            df_job.columns=list(map(lambda x:x.lower(),df_job.columns))\n",
    "\n",
    "            a=list(set(df_user.columns)-(set(df_job.columns)))\n",
    "            print(a)\n",
    "            for i in a:\n",
    "                if(i!='Respondent'):\n",
    "                    df_job[i]=0\n",
    "            b=list(set(df_job.columns)-set(df_user.columns))\n",
    "            print(b)\n",
    "            for i in b:\n",
    "                if(i!='uniq_id'):\n",
    "                    df_user[i]=0\n",
    "            df_user=df_user[sorted(df_user.columns.tolist())]\n",
    "            df_job=df_job[sorted(df_job.columns.tolist())]\n",
    "\n",
    "            print(len(set(df_user.columns).intersection(df_job.columns)),len(df_user.columns))\n",
    "            df_user,df_job=self.clean_common_profile(df_user,df_job,'Database')        \n",
    "            df_user.to_csv(output_path+\"databases_profile_user.csv\")\n",
    "            df_job.to_csv(output_path+\"databases_profile_job.csv\")\n",
    "        #flag indicates that a new user profile\n",
    "    def match_profile(self,input_path,user_id,flag=0):\n",
    "        #Match a given user_id with all jobs in the database\n",
    "        \n",
    "        #Check if user id exists\n",
    "        df=pd.read_csv(input_path+\"domain_user_profile.csv\",index_col='Respondent')\n",
    "        #print(df.columns)\n",
    "        matches=dict()\n",
    "        if(flag==0):\n",
    "            if(user_id in df.index):\n",
    "                userdomain=df.loc[user_id,:]\n",
    "                #print(userdomain)\n",
    "                #If it does, retrieve the user profile from input_path\n",
    "                df=pd.read_csv(input_path+\"languages_profile_user.csv\",index_col='Respondent')\n",
    "                userlanguages=df.loc[user_id,:]\n",
    "\n",
    "                df=pd.read_csv(input_path+\"frameworks_profile_user.csv\",index_col='Respondent')\n",
    "                userframeworks=df.loc[user_id,:]\n",
    "\n",
    "                df=pd.read_csv(input_path+\"platforms_profile_user.csv\",index_col='Respondent')\n",
    "                userplatforms=df.loc[user_id,:]\n",
    "\n",
    "                df=pd.read_csv(input_path+\"databases_profile_user.csv\",index_col='Respondent')\n",
    "                userdatabases=df.loc[user_id,:]\n",
    "\n",
    "                userdomain=np.asarray(userdomain.fillna(0))\n",
    "                userlanguages=np.asarray(userlanguages.fillna(0))\n",
    "                userframeworks=np.asarray(userframeworks.fillna(0))\n",
    "                userplatforms=np.asarray(userplatforms.fillna(0))\n",
    "                userdatabases=np.asarray(userdatabases.fillna(0))\n",
    "                #print(userdomain)\n",
    "            else:\n",
    "                print(\"error! user id not in Dataset\")\n",
    "            #If it doesn't,take user profile as input\n",
    "        else:\n",
    "\n",
    "            print(\"New user!Enter details..\")\n",
    "            name=input(\"Enter full name\")\n",
    "            skills=input(\"Enter skills(comma separated). These are programming languages, frameworks,platforms or databases you have experience with\").split(\",\")\n",
    "            domains=''\n",
    "            flag=1\n",
    "            while(1):\n",
    "                print(\"Enter domain(s) of interest separated by commas(Names are case sensitive). Should be one of the following:\")\n",
    "                for i in df.columns:\n",
    "                    print(i,end=\",\")\n",
    "                domains=input().split(\",\")\n",
    "                for domain in domains:\n",
    "                    if(domain not in df.columns):\n",
    "                        flag=0\n",
    "                        break\n",
    "                if(flag==1):\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Please enter valid domain\")\n",
    "            #domains=list(map(lambda x:x.lower(),domains))\n",
    "            skills=list(map(lambda x:x.lower(),skills))                \n",
    "\n",
    "            userdomain=pd.DataFrame(columns=df.columns)\n",
    "            dictionary=dict()\n",
    "            for domain in domains:\n",
    "                dictionary[domain]=1.0\n",
    "            userdomain=userdomain.append(dictionary,ignore_index=True)\n",
    "\n",
    "\n",
    "            df=pd.read_csv(input_path+\"languages_profile_user.csv\",index_col='Respondent')\n",
    "            userlanguages=pd.DataFrame(columns=df.columns)\n",
    "            dictionary=dict()\n",
    "            for skill in skills:\n",
    "                if(skill in df.columns):\n",
    "                    dictionary[skill]=1.0\n",
    "            userlanguages=userlanguages.append(dictionary,ignore_index=True)\n",
    "\n",
    "            df=pd.read_csv(input_path+\"frameworks_profile_user.csv\",index_col='Respondent')\n",
    "            userframeworks=pd.DataFrame(columns=df.columns)\n",
    "            dictionary=dict()\n",
    "            for skill in skills:\n",
    "                if(skill in df.columns):\n",
    "                    dictionary[skill]=1.0\n",
    "            userframeworks=userframeworks.append(dictionary,ignore_index=True)\n",
    "\n",
    "            df=pd.read_csv(input_path+\"platforms_profile_user.csv\",index_col='Respondent')\n",
    "            userplatforms=pd.DataFrame(columns=df.columns)                \n",
    "            dictionary=dict()\n",
    "            for skill in skills:\n",
    "                if(skill in df.columns):\n",
    "                    dictionary[skill]=1.0\n",
    "            userplatforms=userplatforms.append(dictionary,ignore_index=True)\n",
    "\n",
    "            df=pd.read_csv(input_path+\"databases_profile_user.csv\",index_col='Respondent')\n",
    "            userdatabases=pd.DataFrame(columns=df.columns)               \n",
    "            dictionary=dict()\n",
    "            for skill in skills:\n",
    "                if(skill in df.columns):\n",
    "                    dictionary[skill]=1.0\n",
    "            userdatabases=userdatabases.append(dictionary,ignore_index=True)\n",
    "            #print(userdomain)\n",
    "            userdomain=np.asarray(userdomain.iloc[0,:].fillna(0))\n",
    "            userlanguages=np.asarray(userlanguages.iloc[0,:].fillna(0))\n",
    "            userframeworks=np.asarray(userframeworks.iloc[0,:].fillna(0))\n",
    "            userplatforms=np.asarray(userplatforms.iloc[0,:].fillna(0))\n",
    "            userdatabases=np.asarray(userdatabases.iloc[0,:].fillna(0))\n",
    "                \n",
    "        jobdomain=pd.read_csv(input_path+\"domain_job_profile.csv\",index_col='uniq_id')\n",
    "        joblanguages=pd.read_csv(input_path+'languages_profile_job.csv',index_col='uniq_id')\n",
    "        jobframeworks=pd.read_csv(input_path+'frameworks_profile_job.csv',index_col='uniq_id')\n",
    "        jobplatforms=pd.read_csv(input_path+'platforms_profile_job.csv',index_col='uniq_id')\n",
    "        jobdatabases=pd.read_csv(input_path+'databases_profile_job.csv',index_col='uniq_id')\n",
    "        #print(len(jobdomain.index),len(joblanguages.index))\n",
    "        for i in jobdomain.index:\n",
    "            #print(i)\n",
    "            domain=jobdomain.loc[i,:].fillna(0)\n",
    "            language=joblanguages.loc[i,:].fillna(0)\n",
    "            framework=jobframeworks.loc[i,:].fillna(0)\n",
    "            platform=jobplatforms.loc[i,:].fillna(0)\n",
    "            database=jobdatabases.loc[i,:].fillna(0)\n",
    "            job_id=str(i)\n",
    "            domain=np.asarray(domain)\n",
    "            language=np.asarray(language)\n",
    "            framework=np.asarray(framework)\n",
    "            platform=np.asarray(platform)\n",
    "            database=np.asarray(database)\n",
    "            #print(len(domain),len(userdomain))\n",
    "            score=(0.7*cosine_similarity(domain,userdomain))+(0.3*(cosine_similarity(language,userlanguages)+cosine_similarity(framework,userframeworks)+cosine_similarity(platform,userplatforms)+cosine_similarity(database,userdatabases)))\n",
    "            matches[job_id]=score\n",
    "            score=(0.7*cosine_similarity(domain,userdomain))+(0.3*(cosine_similarity(language,userlanguages)+cosine_similarity(framework,userframeworks)+cosine_similarity(platform,userplatforms)+cosine_similarity(database,userdatabases)))\n",
    "            #Initializing job profiles for later access\n",
    "            self.job_domain=domain\n",
    "            self.job_language=language\n",
    "            self.job_framework=framework\n",
    "            self.job_platform=platform\n",
    "            self.job_database=database\n",
    "            \n",
    "            self.user_domain=userdomain\n",
    "            self.user_language=userlanguages\n",
    "            self.user_framework=userframeworks\n",
    "            self.user_platform=userplatforms\n",
    "            self.user_database=userdatabases\n",
    "        matches=sorted(matches.items(),key=lambda x:x[1],reverse=True)\n",
    "        \n",
    "        recommendations=matches[:10]\n",
    "        #print(\"recommendations are\")\n",
    "        #print(recommendations)\n",
    "        rows=pd.DataFrame(columns=self.df2.columns)\n",
    "        count=0\n",
    "        for i in recommendations:\n",
    "            row=self.df2[self.df2['uniq_id']==i[0]]\n",
    "            #rows[count]=np.asarray(row.values.T.tolist()[0])\n",
    "            rows=rows.append(row.iloc[0])\n",
    "            count=count+1\n",
    "            #print(row)\n",
    "        return rows\n",
    "            \n",
    "\n",
    "obj=job_postings(\"results.csv\")\n",
    "# final_cat=categorize_jobs()\n",
    "# final_cat.to_csv(\"./data/preprocessed_df.csv\")\n",
    "#extracted_skills=obj.clean_skills()\n",
    "#extracted_skills_df=obj.extract_skills(extracted_skills)\n",
    "#print(extracted_skills_df)\n",
    "#domain_df=pd.read_csv(\"./data/preprocessed_df.csv\")\n",
    "#obj.create_job_profile(extracted_skills_df,domain_df)\n",
    "#obj.create_common_profile(\".results/\",\"./data/user_profile/\",\"./data/\")\n",
    "\n",
    "#Path represents the location where final job and user profiles\n",
    "#df_user=pd.read_csv(\"./data/survey_results_public.csv\")\n",
    "#df_job=pd.read_csv(\"results.csv\")\n",
    "\n",
    "#Pass a third parameter(flag) as 1 in order to get your recommendations!\n",
    "rows=obj.match_profile(\"./data/\",3)\n",
    "rows\n",
    "#rows\n",
    "# recommendations_1000=pd.DataFrame(columns=df_job.columns)\n",
    "# for ele in df_user.loc[:,'Respondent'].tolist()[:5000]:\n",
    "#     rows=obj.match_profile(\"./data/\",ele)\n",
    "#     recommendations_1000=recommendations_1000.append(rows.iloc[0,:],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_user=pd.read_csv(\"./data/survey_results_public.csv\",index_col='Respondent')\n",
    "df_user.loc[2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other methods and techniques attempted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv(\"results.csv\")\n",
    "print(df2.head())\n",
    "jobs=[]\n",
    "for job_title  in df2.jobtitle:\n",
    "    if(job_title.lower() not in jobs):\n",
    "        jobs.append(job_title)\n",
    "#print(jobs)\n",
    "job_description=np.asarray(df2.loc[:,\"jobdescription\"])\n",
    "print(len(job_description[0:5]))\n",
    "#nlp=spacy.load('en')\n",
    "\n",
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents=[x for x in doc.ents if not (x.text.isspace())]\n",
    "    return doc\n",
    "#nlp.add_pipe(remove_whitespace_entities,after='ner')\n",
    "#article=job_description[2]\n",
    "#doc=nlp(article)\n",
    "#article=[x for x in nlp(article) if not x.is_stop and x.pos!='PUNCT']\n",
    "#article=[x.lemma_ for x in article]\n",
    "\n",
    "#Using Named Entity Recognition\n",
    "# displacy.render(nlp(str(article)), jupyter=True, style='ent')\n",
    "# nlp(str(article)).ents\n",
    "\n",
    "#Comparing similarity of each word with a given set of words. If the similarity score is high, the word is related to technology. Hence, it should be considered.\n",
    "df_languages=pd.read_excel('results/job_profile/languages.xlsx')\n",
    "df_frameworks=pd.read_excel(\"results/job_profile/frameworks.xlsx\",header=None,error_bad_lines=False,delim_whitespace=True)\n",
    "experience_regex=['\\d+ years','\\d+ experience','']\n",
    "#print(df_frameworks)\n",
    "frame=[str(x).split(\",\")[0] for x in df_frameworks.iloc[:,0]]\n",
    "print(len(df_frameworks.columns))\n",
    "dictionary=list(df_languages.iloc[:,0])\n",
    "dictionary.extend(frame)\n",
    "print(dictionary)\n",
    "dictionary=[x.lower() for x in dictionary]\n",
    "extracted_jobs=dict()\n",
    "for i in range(len(job_description[:10000])):\n",
    "    job_id=df2.iloc[i,-1]\n",
    "    job=df2.iloc[i,3]\n",
    "    flag=0\n",
    "    for word in job.split(\" \"):\n",
    "        word=word.lower()\n",
    "        if word in dictionary:\n",
    "            flag=1\n",
    "            if job_id not in extracted_jobs.keys():\n",
    "                extracted_jobs[job_id]=[]\n",
    "            if word not in extracted_jobs[job_id]:\n",
    "                extracted_jobs[job_id].append(word)\n",
    "    if(flag==0):\n",
    "        print(job_id)\n",
    "print(extracted_jobs)\n",
    "print(len(extracted_jobs))        \n",
    "            \n",
    "#doc_given_text=nlp(u'computer science')\n",
    "#sample_word=nlp(u'Java')\n",
    "#words=[]\n",
    "#for ele in doc:\n",
    "#    if(doc_given_text.similarity(ele)>0.5):\n",
    "#        words.append(ele.text)\n",
    "#print(words)\n",
    "#print(doc_given_text.similarity(sample_word))\n",
    "#print(doc_given_text.similarity(doc[3]))\n",
    "#print(doc[3].vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc10bfb3c641c788952337123b05828afdf19d26"
   },
   "outputs": [],
   "source": [
    "df2=pd.read_csv(\"../input/us-technology-jobs-on-dicecom/dice_com-job_us_sample.csv\")\n",
    "print(df2.head())\n",
    "jobs=[]\n",
    "for job_title  in df2.jobtitle:\n",
    "    if(job_title.lower() not in jobs):\n",
    "        jobs.append(job_title)\n",
    "#print(jobs)\n",
    "job_skills=np.asarray(df2.loc[:,\"skills\"])\n",
    "print(len(job_description[0:5]))\n",
    "#nlp=spacy.load('en')\n",
    "\n",
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents=[x for x in doc.ents if not (x.text.isspace())]\n",
    "    return doc\n",
    "#nlp.add_pipe(remove_whitespace_entities,after='ner')\n",
    "#article=job_description[2]\n",
    "#doc=nlp(article)\n",
    "#article=[x for x in nlp(article) if not x.is_stop and x.pos!='PUNCT']\n",
    "#article=[x.lemma_ for x in article]\n",
    "\n",
    "#Using Named Entity Recognition\n",
    "# displacy.render(nlp(str(article)), jupyter=True, style='ent')\n",
    "# nlp(str(article)).ents\n",
    "\n",
    "#Tokenizing words\n",
    "extracted_skills=dict()\n",
    "training_range=int(0.7*len(job_skills))\n",
    "for i in range(training_range):\n",
    "    #print(i)\n",
    "    #Method 1: Manual pre-processing\n",
    "    job_id=df2.iloc[i,-1]\n",
    "#     job=df2.iloc[i,-2]\n",
    "#     words=job_skills[i].split(\",\")\n",
    "#     if(words[0].lower()==\"(see job description)\"):\n",
    "#         continue\n",
    "#     #print(words)\n",
    "#     #print(len(words))\n",
    "#     for i in range(len(words)):\n",
    "#         #print(type(word))\n",
    "#         chunk=words[i].split(\"/\")\n",
    "#         words.remove(words[i])\n",
    "#         words.extend(chunk)\n",
    "            \n",
    "#     extracted_skills[job_id]=[]\n",
    "#     extracted_skills[job_id].extend(words)\n",
    "    #Method 2:Using NLTK\n",
    "    tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    #print(job_skills[i])\n",
    "    if(pd.isnull(job_skills[i])):\n",
    "        continue\n",
    "    stopwords_list=stopwords.words(\"english\")\n",
    "    tokens=re.split(\"|\".join([\",\",\" and\",\"/\",\" AND\",\" or\",\" OR\",\";\"]),job_skills[i])\n",
    "    tokens=list(set(tokens))\n",
    "    #tokens=[x for x in tokens if x.isalpha()==True]\n",
    "    #tokens=tokenizer.tokenize(job_skills[i])\n",
    "    #print(tokens)\n",
    "    #tokens=nltk.word_tokenize(job_skills[i])\n",
    "    #print(stopwords)\n",
    "    #stopwords_list=stopwords\n",
    "    #words=[x for x in tokens if x not in stopwords_list]\n",
    "    #print(tokens)\n",
    "    extracted_skills[job_id]=[]\n",
    "    extracted_skills[job_id].extend(tokens)\n",
    "    #print(extracted_skills[job_id])\n",
    "print(extracted_skills)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ac15adc6a748dc3813694a054a1b2e7909a3ade"
   },
   "source": [
    "Part A: Building the content analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4403e95e5d0c3ebea3e178e9b9d03c72403c35e7"
   },
   "outputs": [],
   "source": [
    "#Identifying various categories in job postings\n",
    "\n",
    "#Method1: Applying TF-IDF on the dataset\n",
    "count=0\n",
    "docs=[]\n",
    "for i in range(len(job_description[:100])):\n",
    "    #print(job_description[i])\n",
    "    if(job_description[i]==np.nan):\n",
    "        continue\n",
    "    doc=[x for x in job_description[i].split(\" \") if x not in stopwords_list]\n",
    "    docs.append(\" \".join(doc))\n",
    "print(len(docs))\n",
    "vectorizer=TfidfVectorizer(ngram_range=(1,2),max_df=0.6,max_features=50)\n",
    "response=vectorizer.fit_transform(docs)\n",
    "name_to_index=vectorizer.get_feature_names()\n",
    "response=response.toarray()\n",
    "scores=pd.DataFrame(data=response[:,:],index=range(len(response)),columns=name_to_index)\n",
    "print(scores)\n",
    "max_col_scores={}\n",
    "#print(scores.iloc[0,:])\n",
    "for col in range(len(scores.iloc[0,:])):\n",
    "    col_score=sum(scores.iloc[:,col])\n",
    "    max_col_scores[name_to_index[col]]=col_score\n",
    "max_col_scores=sorted(max_col_scores.items(),reverse=True,key=lambda x:x[1])[:50]\n",
    "print(max_col_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3afbe12e20b0df23d430a016bb4bf2bacdcce76"
   },
   "outputs": [],
   "source": [
    "#Method 2: Use a separate domain field in the dataset.\n",
    "#Use TF-DF on job titles\n",
    "def cluster_job_titles():\n",
    "    job_titles=df2.loc[:,'jobtitle'].tolist()\n",
    "    #Tokenization   \n",
    "    \n",
    "    docs=[]\n",
    "    for i in range(len(job_titles[:training_range])):\n",
    "        #print(job_description[i])\n",
    "        if(job_titles[i]==np.nan):\n",
    "            continue\n",
    "        doc=[x for x in job_description[i].split(\" \") if x not in stopwords_list]\n",
    "        docs.append(\" \".join(doc))\n",
    "    print(len(docs))\n",
    "    vectorizer=TfidfVectorizer(ngram_range=(1,2),max_df=1.0,max_features=50)\n",
    "    response=vectorizer.fit_transform(docs)\n",
    "    model=KMeans(n_clusters=10,init='k-means++')\n",
    "    model.fit(response)\n",
    "    labels=model.labels_\n",
    "    return labels\n",
    "#name_to_index=vectorizer.get_feature_names()\n",
    "#response=response.toarray()\n",
    "#scores=pd.DataFrame(data=response[:,:],index=range(len(response)),columns=name_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ef3577052cd750b7e49cf622164dd8f4ccee4d5"
   },
   "outputs": [],
   "source": [
    "# labels=cluster_job_titles()\n",
    "# cluster_mapping=dict()\n",
    "# for i in range(len(labels)):\n",
    "#     label=labels[i]\n",
    "#     if(label not in cluster_mapping.keys()):\n",
    "#         cluster_mapping[label]=[]\n",
    "#     cluster_mapping[label].append(job_titles[i])\n",
    "# print(cluster_mapping)\n",
    "# print(len(cluster_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60298c92af63937bfd75c9b97213a56fe586f777"
   },
   "outputs": [],
   "source": [
    "# #Predefined categories\n",
    "# #Compare similarities of word embeddings\n",
    "# nlp=spacy.load('en_core_web_lg')\n",
    "# # jobs=df2.loc[:,'jobtitle'].tolist()[:50]\n",
    "# # jobs_descriptions=df2.loc[:,'jobdescription'].tolist()[:50]\n",
    "# # for job in jobs:\n",
    "# #     #print(job)\n",
    "# #     doc=nlp(job)\n",
    "# #     print(doc.ents)\n",
    "#     #print(\"Entity is\",ele.text,\"Label\",ele.label_)\n",
    "# def check_threshold(threshold,ele):\n",
    "#     if(ele[0]!=threshold[0][0] and abs(ele[1]-threshold[0][1])<0.03 and ele[1]>0.5):\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "# def categorize(training_range):\n",
    "#     job_id=df2.loc[:,'uniq_id'].tolist()[:training_range]\n",
    "#     job_titles=df2.loc[:,'jobtitle'].tolist()[:training_range]\n",
    "#     job_descriptions=df2.loc[:,'jobdescription'].tolist()[:training_range]\n",
    "#     final_cat=pd.DataFrame(index=job_id)\n",
    "#     #categories=['Network Engineer','Application Development','Big Data','Data Analyst','Software Developer','DevOps','Software Testing','Front End','Back End','Full Stack','Web Development','Information Security','Mobile developer','System Administrator','Business Analyst','Manager','Cloud']\n",
    "#     categories=['Network Engineer','Full stack','QA/Test Developer','Enterprise application','DevOps','Mobile Developer','Back End','Database Administrator(DBA)','Front End','Game developer','System Administrator','Data Scientist','Business analyst','Sales professional','Product Manager','Information Security','Software Developer/Java Developer','Web Developer']\n",
    "#     for category in categories:\n",
    "#         final_cat[category]=np.nan\n",
    "#     for job_t_d in list(zip(job_id,job_titles,job_descriptions)):\n",
    "#         id_job=job_t_d[0]\n",
    "#         job_i=job_t_d[1]\n",
    "#         job_d=job_t_d[2]\n",
    "#         job_title=nlp(job_i.lower())\n",
    "#         job_description=nlp(job_d.lower())\n",
    "#         match_cat_title=dict()\n",
    "#         match_cat_description=dict()\n",
    "#         for category in categories:\n",
    "#             word=nlp(category.lower())\n",
    "#             match_cat_title[category]=job_title.similarity(word)\n",
    "#             match_cat_description[category]=job_description.similarity(word)\n",
    "#         match_cat_title=sorted(match_cat_title.items(),key=lambda x:x[1],reverse=True)\n",
    "#         match_cat_description=sorted(match_cat_description.items(),key=lambda x:x[1],reverse=True)\n",
    "        \n",
    "        \n",
    "#         #a represents max\n",
    "#         if(match_cat_title[0][1]>0.5 or match_cat_description[0][1]>0.5):\n",
    "#             a=match_cat_title[0]\n",
    "#             print(a)\n",
    "#             match_cat_description=list(filter(lambda x: check_threshold(match_cat_title,x),match_cat_description))\n",
    "#             if(len(match_cat_description)!=0):\n",
    "#                 print(match_cat_description)\n",
    "#                 print(id_job)\n",
    "#                 #b=match_cat_description[0]\n",
    "#                 final_cat.loc[id_job,a[0]]=1\n",
    "#                 match_cat_description.extend([(match_cat_title[0][0],1)])\n",
    "#                 sum_proportion=sum([x[1] for x in match_cat_description])\n",
    "#                 for ele in match_cat_description:\n",
    "#                     final_cat.loc[id_job,ele[0]]=ele[1]/sum_proportion\n",
    "#             else:\n",
    "#                 print(id_job)\n",
    "#                 final_cat.loc[id_job,a[0]]=1\n",
    "\n",
    "#         else:\n",
    "#             print(\"not considering\",job_i)\n",
    "#         #print(match_cat)\n",
    "#     return final_cat\n",
    "# training_range=int(0.7*len(df2.loc[:,'uniq_id']))\n",
    "# final_cat=categorize(training_range)\n",
    "\n",
    "# print(final_cat)\n",
    "# final_cat.to_csv(\"preprocessed_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e7658ea046e4f5876928a63aaeb9cebdb0bafbf"
   },
   "outputs": [],
   "source": [
    "#Extracting years of experience\n",
    "nlp=spacy.load('en_core_web_lg')\n",
    "job_description=df2.loc[:,'jobdescription'].tolist()\n",
    "id_job=df2.loc[:,'uniq_id'].tolist()\n",
    "experience_regex=['\\d+ years \\w+ $\\.',r'\\d+ experience']\n",
    "matches=dict()\n",
    "entities=dict()\n",
    "for job_id,description in list(zip(id_job,job_description))[:10]:\n",
    "    #l=re.findall(r\"\\w* experience[\\S*\\s*]\\w*[.]\",description)\n",
    "    l=re.findall(r\"[^.]*experience[^.]*\\.\",description)\n",
    "    matches[job_id]=l    \n",
    "    for string in matches[job_id]:\n",
    "        print(string)\n",
    "        doc=nlp(string)\n",
    "        \n",
    "        for token in doc:\n",
    "            print(token.text,token.dep_,token.head.text)\n",
    "print(matches)\n",
    "#print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc1c4c8814ec65c881476d2f8d83fa0e2319d74a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b24c1a19e23e3bde4d92103a08a65420bca1d14"
   },
   "outputs": [],
   "source": [
    "#Identifying "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca2a373b2d99422ee8ba00f77883d3446dafa9c7"
   },
   "source": [
    "Part B: Profile Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0cb0b32b64691045907a06bbe2b0a34f92ea8ff9"
   },
   "outputs": [],
   "source": [
    "#Explicit/Implicit ways of identifying user preferences\n",
    "#1) Use a like/dislike method to indicate user preference:\n",
    "#Optimal method: Model based user-preference method to infer a score for each job(item) that the user has worked for in the past "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinkedIn Collab\n",
    "files=glob.glob(\"../scraped profiles/*.json\")\n",
    "#print(files)\n",
    "for file in files[:1]:\n",
    "    f=open(\"../scraped profiles/\"+file)\n",
    "    data=json.load(f)\n",
    "    print(data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
